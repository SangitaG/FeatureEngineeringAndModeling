{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook, Step 4 - Build Model\n",
    "\n",
    "Implement your final model\n",
    "(Optionally) use the entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split   \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,precision_recall_curve,confusion_matrix \n",
    "from sklearn.metrics import (precision_score,accuracy_score,roc_auc_score,roc_curve, \n",
    "                             precision_recall_curve,recall_score,make_scorer,auc) \n",
    "from scipy.stats import boxcox, skew\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Read_pickled_madelon_Datasets(object):\n",
    "   \n",
    "    # Initializing object.\n",
    "    def __init__(self):\n",
    "       \n",
    "        # Read pickled data from NB1.\n",
    "        self.data_dir = '../dataNB1_DB/'    \n",
    "\n",
    "        self.read_all_data_from_files()\n",
    "        \n",
    "    def read_all_data_from_files(self):\n",
    "        # Read train data.\n",
    "        self.X_train_data_df_1 = pd.read_pickle(self.data_dir +'X_train_1_df.p')\n",
    "        self.X_train_data_df_2 = pd.read_pickle(self.data_dir +'X_train_2_df.p')\n",
    "        self.X_train_data_df_3 = pd.read_pickle(self.data_dir +'X_train_3_df.p')\n",
    "\n",
    "        # X test data.\n",
    "        self.X_test_df = pd.read_pickle(self.data_dir +'X_test_df.p')\n",
    "        \n",
    "        # X test data.\n",
    "        self.X_validate_df = pd.read_pickle(self.data_dir +'X_validate_df.p')\n",
    "\n",
    "        # y train data.\n",
    "        self.y_train_data_1 = pd.read_pickle(self.data_dir + 'y_train_1.p')\n",
    "        self.y_train_data_2 = pd.read_pickle(self.data_dir + 'y_train_2.p')\n",
    "        self.y_train_data_3 = pd.read_pickle(self.data_dir + 'y_train_3.p')\n",
    "        \n",
    "        # y test data.\n",
    "        self.y_test_data_1 = pd.read_pickle(self.data_dir + 'y_test.p')\n",
    " \n",
    "    def get_X_train_data_sets(self):\n",
    "        return(self.X_train_data_df_1, self.X_train_data_df_2, self.X_train_data_df_3)\n",
    "    \n",
    "    def get_X_valid_data(self):\n",
    "        return(self.X_validate_df)\n",
    "    \n",
    "\n",
    "    def get_X_test_data(self):\n",
    "        return(self.X_test_df)\n",
    "       \n",
    "    def get_y_train_data(self):        \n",
    "        # y data should be a 1D array of labels. It is now read as a dataframe of 1 column with each\n",
    "        # element as a list of one int.  We want it to be just an int not a list.\n",
    "        self.y_train_data_1 = np.array([val[0] for val in self.y_train_data_1.values])\n",
    "        self.y_train_data_2 = np.array([val[0] for val in self.y_train_data_2.values])\n",
    "        self.y_train_data_3 = np.array([val[0] for val in self.y_train_data_3.values])\n",
    "        \n",
    "        return(self.y_train_data_1, self.y_train_data_2, self.y_train_data_3)\n",
    "    \n",
    "    def get_y_test_data(self):\n",
    "        # y data should be a 1D array of labels. It is now read as a dataframe of 1 column with each\n",
    "        # element as a list of one int.  We want it to be just an int not a list.\n",
    "        self.y_test = np.array([val[0] for val in self.y_test_data_1.values])\n",
    "        \n",
    "        return(self.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in DB Madelon Data. For this notebook we only need the X and y train data.\n",
    "read_data = Read_pickled_madelon_Datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X train data.\n",
    "X_train_data_df_1, X_train_data_df_2, X_train_data_df_3 = \\\n",
    "                                                        read_data.get_X_train_data_sets()\n",
    "# y train data.\n",
    "y_train_1, y_train_2, y_train_3 = read_data.get_y_train_data()\n",
    "\n",
    "# X train datasets in a list.\n",
    "X_train_subsets = [X_train_data_df_1, X_train_data_df_2, X_train_data_df_3]\n",
    "\n",
    "# y train datasets in a list.\n",
    "y_train_subsets = [y_train_1, y_train_2, y_train_3]\n",
    "\n",
    "# Read X and y test data.\n",
    "Xtest = read_data.get_X_test_data()\n",
    "ytest = read_data.get_y_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read pickled data subsets with only the selected features.\n",
    "\n",
    "# Read pickled data.\n",
    "X_train_sel_feats_subsets_df=[]\n",
    "X_train_sel_feats_subsets_df.append(pd.read_pickle('../dataNB3_DB/X_train_sel_feats_subsets_df1.p'))\n",
    "X_train_sel_feats_subsets_df.append(pd.read_pickle('../dataNB3_DB/X_train_sel_feats_subsets_df2.p'))\n",
    "X_train_sel_feats_subsets_df.append(pd.read_pickle('../dataNB3_DB/X_train_sel_feats_subsets_df3.p'))\n",
    "\n",
    "Xtest_sel_feats = pd.read_pickle('../dataNB3_DB/X_test_sel_feats_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results_df = pd.read_pickle('../dataNB3_DB/FinalModels_GOOD_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>sc_tr_acc_score</th>\n",
       "      <th>sc_tst_acc_score</th>\n",
       "      <th>sc_precision</th>\n",
       "      <th>sc_recall</th>\n",
       "      <th>pipe_object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Set_1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.59850</td>\n",
       "      <td>0.597333</td>\n",
       "      <td>0.587450</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Set_1</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.95375</td>\n",
       "      <td>0.791333</td>\n",
       "      <td>0.782086</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Set_1</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.73125</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>0.663564</td>\n",
       "      <td>0.678912</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Set_1</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.792667</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.794558</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Set_2</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.60425</td>\n",
       "      <td>0.595333</td>\n",
       "      <td>0.587432</td>\n",
       "      <td>0.585034</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Set_2</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.94775</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>0.798141</td>\n",
       "      <td>0.817687</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Set_2</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.77425</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.685567</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Set_2</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.801333</td>\n",
       "      <td>0.784135</td>\n",
       "      <td>0.820408</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Set_3</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.59750</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.580769</td>\n",
       "      <td>0.616327</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Set_3</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.95300</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.967347</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Set_3</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.75650</td>\n",
       "      <td>0.742000</td>\n",
       "      <td>0.716958</td>\n",
       "      <td>0.782313</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Set_3</td>\n",
       "      <td>KNN</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset               model  sc_tr_acc_score  sc_tst_acc_score  \\\n",
       "0    Set_1  LogisticRegression          0.59850          0.597333   \n",
       "1    Set_1                 SVC          0.95375          0.791333   \n",
       "2    Set_1        DecisionTree          0.73125          0.674000   \n",
       "3    Set_1                 KNN          1.00000          0.792667   \n",
       "4    Set_2  LogisticRegression          0.60425          0.595333   \n",
       "5    Set_2                 SVC          0.94775          0.809333   \n",
       "6    Set_2        DecisionTree          0.77425          0.702000   \n",
       "7    Set_2                 KNN          1.00000          0.801333   \n",
       "8    Set_3  LogisticRegression          0.59750          0.594000   \n",
       "9    Set_3                 SVC          0.95300          0.960000   \n",
       "10   Set_3        DecisionTree          0.75650          0.742000   \n",
       "11   Set_3                 KNN          1.00000          1.000000   \n",
       "\n",
       "    sc_precision  sc_recall                                        pipe_object  \n",
       "0       0.587450   0.598639  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "1       0.782086   0.795918  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "2       0.663564   0.678912  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "3       0.784946   0.794558  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "4       0.587432   0.585034  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "5       0.798141   0.817687  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "6       0.685567   0.723810  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "7       0.784135   0.820408  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "8       0.580769   0.616327  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "9       0.951807   0.967347  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "10      0.716958   0.782313  Pipeline(steps=[('standardscaler', StandardSca...  \n",
       "11      1.000000   1.000000  Pipeline(steps=[('standardscaler', StandardSca...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gridsearch results from Notebook3. The gridsearch was performed on 3 sets of data.\n",
    "\n",
    "model_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_df.pipe_object[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results = []\n",
    "target_names=['class 0', 'class 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I will use datasets 2 and 3 appended together as my X train data. That will be 8000 rows.\n",
    "# This X data has been reduced to only the selected features. We will perform PCA on ot.\n",
    "Xtrain = X_train_sel_feats_subsets_df[0].append(X_train_sel_feats_subsets_df[1]).append(X_train_sel_feats_subsets_df[2]) \n",
    "\n",
    "# y train will be y data for subset 2 and 3 appended together.\n",
    "ytrain = np.append(np.append(y_train_subsets[0], y_train_subsets[1]),y_train_subsets[2])\n",
    "\n",
    "# X test and y test are pickled data that I set aside for this final step. They are 1500 rows.\n",
    "Xtest = Xtest_sel_feats\n",
    "\n",
    "ytest = ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12000, 19), (12000,), (1500, 19), (1500,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* KNN ********\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.88      0.85      0.86       765\n",
      "    class 1       0.85      0.88      0.86       735\n",
      "\n",
      "avg / total       0.86      0.86      0.86      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for tst_set in tqdm(range(1)):                                     \n",
    "\n",
    "    # FINAL MODEL: KNN pipes.\n",
    "    KNN_scaled_pipe = make_pipeline(StandardScaler(), \n",
    "                                    PCA(),\n",
    "                                    KNeighborsClassifier(n_neighbors = 5, weights='uniform'))\n",
    "                                               \n",
    "    # Fit and score pipeline.\n",
    "    KNN_scaled_pipe.fit(Xtrain, ytrain) \n",
    "    \n",
    "    # Mean accuracy score is returned.\n",
    "    train_score = KNN_scaled_pipe.score(Xtrain,ytrain)\n",
    "    test_score = KNN_scaled_pipe.score(Xtest,ytest)  \n",
    "    y_pred = KNN_scaled_pipe.predict(Xtest)\n",
    "    \n",
    "    # Calculate precision and recall.   \n",
    "    recall = recall_score(ytest, y_pred)\n",
    "    precision = precision_score(ytest, y_pred)\n",
    "    \n",
    "    output = {'train_score': train_score, \n",
    "              'test_score' : test_score, \n",
    "              'recall'     : recall, \n",
    "              'precision'  : precision,\n",
    "              'model_name' : 'KNN'}\n",
    "    \n",
    "    model_results.append(output)\n",
    "    \n",
    "    #classification reports. \n",
    "    class_report_KNN_str = classification_report(ytest, y_pred, target_names=target_names)   \n",
    "\n",
    "    print(\"********* KNN ********\\n\\n\", class_report_KNN_str)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.850727</td>\n",
       "      <td>0.87619</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.873167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  precision   recall  test_score  train_score\n",
       "0        KNN   0.850727  0.87619       0.864     0.873167"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
